{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dtale\n",
    "home_dir=os.path.expanduser('~')\n",
    "os.chdir(f\"{home_dir}/nexus_correlation_discovery/\")\n",
    "from demo import nexus_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nexus Introduction\n",
    "\n",
    "Correlation analysis is a vital initial step for investigating causation, essential for understanding complex phenomena and making informed choices. While it is hard to establish causality from vast observational data without assumptions and expert knowledge, identifying correlations remains a key strategy to “cast a wide net” and detect potential causal links. Our system Nexus identifies correlations over collections of spatio-temporal tabular data, aiming to identify interesting hypotheses and provide a good starting point for further causal analysis. Nexus focuses on two personas.\n",
    "\n",
    "**Persona 1: Exploring an Existing Hypothesis.** A researcher at a medical school, Bob, has a dataset with asthma attack incidences in hospitals across various zip codes in Chicago. Bob's research goal is to explore what factors could potentially affect asthma attacks. Thus, he wants to start by finding variables that are correlated with asthma attacks. Persona 1 is someone who has an initial dataset and seeks to enrich such a dataset with additional variables relevant to the analysis.\n",
    "\n",
    "<img src=\"persona1.png\" alt=\"persona 1\" width=\"500\"/>\n",
    "\n",
    "**Persona 2: Data-Driven Hypothesis Generation.** Amy, a researcher in Chicago, finds [Chicago Open Data](https://data.cityofchicago.org) has many interesting datasets. She want to know whether she could form new hypotheses from BIG DATA. That is to find all correlations in Chicago Open Data and see if there is any interesting ones that can lead to new hypotheses or insights.\n",
    "\n",
    "<img src=\"persona2.png\" alt=\"persona 2\" width=\"400\"/>\n",
    "\n",
    "In this demonstration, we will illustrate how Nexus assists Persona 1 and 2 with the analysis of real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Nexus\n",
    "\n",
    "Let's first install Nexus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:58:00.810719834Z",
     "start_time": "2024-02-26T22:58:00.805785609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation Nexus successful!\n"
     ]
    }
   ],
   "source": [
    "nexus_demo.install_nexus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Nexus API\n",
    "\n",
    "Nexus indexes Chicago Open Data offline and stores the data in `quickstart.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexus.nexus_api import API\n",
    "# conn_str = f'data/quickstart.db'\n",
    "conn_str = f'data/demo.db'\n",
    "nexus_api = API(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona 1: Enrich the asthma dataset with additional variables\n",
    "\n",
    "Bob, a researcher from a medical school, has a dataset with asthma attack incidences in hospitals across various zip codes in Chicago.\n",
    "\n",
    "| Zip5\\*   | enc_asthma\\*\\* | encAsthmaExac\\*\\*\\* | AttackPer\\*\\*\\*\\*  |\n",
    "|--------|------------|---------------|-----------|\n",
    "| 60604.0| 10.0       | 1.0           | 0.1       |\n",
    "| 60605.0| 47.0       | 7.0           | 0.15      |\n",
    "| 60606.0| 33.0       | 13.0          | 0.39      |\n",
    "| 60607.0| 12.0       | 3.0           | 0.25      |\n",
    "| ...| ...       | ...          | ...      |\n",
    "\n",
    "\\* zipcode\n",
    "\n",
    "\\*\\* Count of asthma visits 2009-2019, denominator.\n",
    "\n",
    "\\*\\*\\* Count of visits for asthma attacks (a.k.a., exacerbations) 2009-2019, numerator.\n",
    "\n",
    "\\*\\*\\*\\* Asthma attacks as a percentage of all asthma visits.\n",
    "\n",
    "Bob wants to find variables correlated with asthma attacks from Chicago Open Data. \n",
    "\n",
    "<!-- He finds that [Chicago Open Data](https://data.cityofchicago.org/) has a wealth of datasets on diverse societal aspects such as education, business, and crime in Chicago. He believes there are some variables in Chicago Open Data that are useful for his research. Thus, he adds Chicago Open Data as a data source in Nexus. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Data Assets\n",
    "\n",
    "Chicago Open Data has been added to Nexus and Bob can use Nexus to browse the data catalog. \n",
    "\n",
    "Note this data catalog contains both the original dataset and their aggregated version. For example, table `ijzp-q8t2` is Crimes - 2001 to Present. This table originally has geo-coordinate granularity. To combine it with the asthma dataset having zipcode granularity, Nexus automatically resolves the granularity inconsistency and creates table `ijzp-q8t2_location_6` that aggregates ijzp-q8t2 to the zipcode granularity using the `location` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Yues-MacBook-Pro.local:40000/dtale/iframe/19\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x3125f9190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog = nexus_api.get_catalog()\n",
    "dtale.show(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Nexus to look at a dataset in the catalog given the dataset id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Yues-MacBook-Pro.local:40000/dtale/iframe/10\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x3176ffb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = 'ijzp-q8t2_location_6'\n",
    "df = nexus_api.get_agg_dataset(dataset_id)\n",
    "dtale.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find correlations from an input table\n",
    "\n",
    "Bob's goal is to explore what factors could potentially affect asthma attacks. Thus, he starts by finding variables that are correlated with asthma attacks. He can achieve this easily by using the `find_correlations_from` API in Nexus.\n",
    "\n",
    "In this API, Nexus aligns the asthma dataset with tables from Chicago Open Data and computes correlations. Tables from Chicago Open Data originally have the spatial granularity of geo-coordinate. We aggregate them to the zip code level and apply aggregate functions \"avg\" and \"count\". For example, if you see an attribute named `avg_basketball_courts`, it means the original attribute is `basketball_courts` and function `average` is applied. The attribute after aggregation is named `avg_basketball_courts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of correlations: 219\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Yues-MacBook-Pro.local:40000/dtale/iframe/11\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x313c1c050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nexus.utils.time_point import TEMPORAL_GRANU\n",
    "from nexus.utils.coordinate import SPATIAL_GRANU\n",
    "\n",
    "dataset = 'asthma'\n",
    "# dataset = 'ijzp-q8t2'\n",
    "# asthma data only has spatial attribute, thus the temporal granularity is set to ALL.\n",
    "temporal_granularity, spatial_granularity = None, SPATIAL_GRANU.ZIPCODE\n",
    "# temporal_granularity, spatial_granularity = TEMPORAL_GRANU.MONTH, SPATIAL_GRANU.ZIPCODE\n",
    "overlap_threshold = 5\n",
    "correlation_threshold = 0.5\n",
    "# you can change correlation_type to 'spearman' or 'kendall'\n",
    "correlations = nexus_api.find_correlations_from(dataset, temporal_granularity, spatial_granularity, \n",
    "                                      overlap_threshold, correlation_threshold, \n",
    "                                      correlation_type=\"pearson\")\n",
    "dtale.show(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the detailed profile of a correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 1 - table id: asthma, aggregated table: asthma_Zip5_6, aggregated attribute: avg_encAsthmaExac\n",
      "\t Missing value ratio: 0.0\n",
      "\t zero value ratio: 0.02\n",
      "Variable 2 - table id: 9xs2-f89t, aggregated table: 9xs2-f89t_location_6, aggregated attribute: avg_general_services_route_\n",
      "\t Missing value ratio: 0.0\n",
      "\t zero value ratio: 0.0\n",
      "Correlation Profile\n",
      "\tCorrelation coefficient: 0.603\n",
      "\tp value: 0.0\n",
      "\tNumber of samples: 46\n",
      "\tSpatio-temporal key type: spatial\n"
     ]
    }
   ],
   "source": [
    "correlation_idx = 9\n",
    "nexus_api.show_correlation_profile(correlations, correlation_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control for variables\n",
    "\n",
    "Bob got more than 200 correlations for the asthma dataset. After browsing several correlations, he realizes that \"poverty\" might be driving these correlations. Thus, he wants to control for the income level of each zipcode when calculating correlations. To achieve that, users can specify variables that they want to control in the `control_variables` parameter. After controlling for the median household income in a zipcode, only 60 correlations are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of correlations: 21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Yues-MacBook-Pro.local:40000/dtale/iframe/18\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x318669190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nexus.utils.data_model import Variable\n",
    "\n",
    "dataset = 'asthma'\n",
    "temporal_granularity, spatial_granularity = None, SPATIAL_GRANU.ZIPCODE\n",
    "overlap_threshold = 5\n",
    "correlation_threshold = 0.5\n",
    "control_variables = [Variable('chicago_income_by_zipcode_zipcode_6', 'avg_income_household_median')]\n",
    "df_control = nexus_api.find_correlations_from(dataset, temporal_granularity, spatial_granularity, \n",
    "                                              overlap_threshold, correlation_threshold, \n",
    "                                              correlation_type=\"spearman\", control_variables=control_variables)\n",
    "dtale.show(df_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble a dataset from multiple variables\n",
    "\n",
    "Bob identifies a few interesting correlations and wants to combine variables involved in these correlations to assemble a new dataset. Nexus provides data assembly APIs to make it easy for Bob.\n",
    "\n",
    "Suppose Bob finds the first correlation intriguing and wishes to explore the data used to calculate it. In such a scenario, he can simply input the correlation's ID into Nexus to obtain the integrated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Yues-MacBook-Pro.local:40000/dtale/iframe/13\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x3124504d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_idx = 40\n",
    "aligned, prov = nexus_api.get_joined_data_from_row(df_control.loc[row_idx])\n",
    "dtale.show(aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus also offers `join_and_project` API that can assemble a dataset from any set of given variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Yues-MacBook-Pro.local:40000/dtale/iframe/14\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x317639110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = [Variable('divg-mhqk_location_6', 'count'), Variable('4u6w-irs9_location_6', 'avg_square_feet')]\n",
    "df, prov = nexus_api.join_and_project(variables)\n",
    "dtale.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus provides the data provenance information for all data assembly APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"divg-mhqk_location_6\".count,\"4u6w-irs9_location_6\".avg_square_feet FROM \"divg-mhqk_location_6\" INNER JOIN \"4u6w-irs9_location_6\" ON \"divg-mhqk_location_6\".val = \"4u6w-irs9_location_6\".val\n"
     ]
    }
   ],
   "source": [
    "print(prov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis\n",
    "\n",
    "When you find multiple intriguing correlations and wish to conduct further regression analysis on variables of interest, you can begin by utilizing Nexus's `join_and_project` function to compile the necessary dataset. Subsequently, you may employ any data analysis library for regression analysis. In this instance, we will illustrate the process using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients of each independent variables: [ 3.17139472e-02 -5.13593106e+02]\n",
      "r square score: 0.3408732623177211\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "dependent_variable = Variable('asthma_Zip5_6', 'avg_enc_asthma')\n",
    "independent_variables = [Variable('ijzp-q8t2_location_6', 'count'), Variable('n26f-ihde_pickup_centroid_location_6', 'avg_tip')]\n",
    "\n",
    "data_to_analyze, provenance = nexus_api.join_and_project([dependent_variable] + independent_variables)\n",
    "# apply any data anlysis method\n",
    "regression_model = linear_model.LinearRegression() # OLS regression\n",
    "\n",
    "x = data_to_analyze[[variable.attr_name for variable in independent_variables]]\n",
    "y = data_to_analyze[dependent_variable.attr_name]\n",
    "model = regression_model.fit(x, y)\n",
    "r_squared = model.score(x, y)\n",
    "\n",
    "print(\"coefficients of each independent variables:\", model.coef_)\n",
    "print(\"r square score:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona 2: Data-Driven Hypothesis Generation.\n",
    "Amy, a researcher in Chicago, finds [Chicago Open Data](https://data.cityofchicago.org) has many interesting datasets. She want to know whether she could form new hypotheses from BIG DATA. That is to find all correlations in Chicago Open Data and see if there is any interesting ones that can lead to new hypotheses or insights.\n",
    "\n",
    "She can use the `find_all_correlations` API to identify all correlations within Chicago Open Data at the census tract and month granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nexus found 40538 correlations in total\n"
     ]
    }
   ],
   "source": [
    "from nexus.utils.time_point import TEMPORAL_GRANU\n",
    "from nexus.utils.coordinate import SPATIAL_GRANU\n",
    "chicago_correlations = nexus_demo.find_all_correlations(TEMPORAL_GRANU.MONTH, SPATIAL_GRANU.TRACT)\n",
    "print(f\"Nexus found {len(chicago_correlations)} correlations in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Distillation Using Nexus Variable Clusters\n",
    "\n",
    "Nexus found 40,538 correlations in total, which is an overwhelming number for users to discern interesting correlations manually.\n",
    "\n",
    "Luckily, Nexus can distill the structure of correlations and extract a small number of variable clusters from the vast array of correlations. These variable clusters can help users identify causal links and confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nexus extracts 23 variable clusters out of 40538 correlations\n"
     ]
    }
   ],
   "source": [
    "from demo.cluster_utils import CorrCommunity\n",
    "from demo.demo_ui import show_communities\n",
    "\n",
    "variable_clusters = nexus_demo.get_correlation_communities(chicago_correlations)\n",
    "print(f\"Nexus extracts {len(variable_clusters.comps)} variable clusters out of {len(chicago_correlations)} correlations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Correlation Communities\n",
    "\n",
    "Nexus helps Amy reduce the burden of examining correlations by extracting 23 clusters from the vast correlations. \n",
    "\n",
    "There is a cluster (Cluster 14) with tables related to divvy bike stations, taxi trips, and Chicago covid-19 community vulnerability index (CCVI). CCVI score measures a community’s susceptibility to the negative impacts from COVID-19 based on various social and economic factors. A lower CCVI score means less vulnerability, indicating an area has a more advanced socio-economic status.\n",
    "\n",
    "These significant negative correlations between CCVI score and divvy bike docks inspire Amy to form a hypothesis that Divvy bike locations are biased towards richer areas. Notably, this hypothesis has been verified in existing studies [1]. \n",
    "\n",
    "[1] Elizabeth Flanagan and et al. 2016. Riding tandem: Does cycling infrastructure investment mirror gentrification and privilege in Portland, OR and Chicago, IL? Research in Transportation Economics 60 (2016), 14–24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadaf8e9c94a45448d7eb3a1de8c0d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Show:', layout=Layout(width='200px'), options=('Cluster 0', 'Cluster 1', 'Cluster 2', 'C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafe80d030e14fdfac9652192adb999d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797e9404296741e0a1521ebbc81bd0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afb7f12f7ac4f72b407c40e9dc49b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_communities(variable_clusters, show_corr_in_same_tbl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on using factor analysis\n",
    "\n",
    "Factor analysis aims to extract common factors from observed variables and represent existing variables using fewer factors. \n",
    "\n",
    "It takes as input a correlation matrix. It derives factors that are essentially linear combinations of the observed variables. These factors are crafted to closely approximate the original correlation matrix when observed variables are projected onto them. \n",
    "\n",
    "We also implement factor analysis in Nexus, but it has several limitations when applied on a large correlation matrix:\n",
    "\n",
    "1. Assumption. Factor analysis assumes these correlations among observed variables are computed on the same set of samples. However, in our scenario, variables are from different datasets and aligned on different samples, which breaks this assumption.\n",
    "\n",
    "2. Scalability Issue. Factor analysis does matrix decomposition and its runtime grows quadratically. It runs for 10 minutes on 556 variables.\n",
    "\n",
    "3. Hard to determine the number of factors. Although there are some methods to choose the factors, they do not work well on large correlation matrices. For example, the most used method is to look at the eigenvalues of the correlation matrix and select the number of eigenvalues greater than 1 as the number of factors. I tried this method and found we needed 253 factors! A human can hardly interpret that many factors.\n",
    "\n",
    "4. Hard to determine the threshold for assigning variables to factors. There is no golden rule to determine this threshold. As a rule of thumb, 0.7 or higher factor loading represents that the factor extracts sufficient variance from that variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
