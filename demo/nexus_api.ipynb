{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:58:00.810719834Z",
     "start_time": "2024-02-26T22:58:00.805785609Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "home_dir=os.path.expanduser('~')\n",
    "os.chdir(f\"{home_dir}/nexus_correlation_discovery/\")\n",
    "from utils.time_point import TEMPORAL_GRANU\n",
    "from utils.coordinate import SPATIAL_GRANU\n",
    "from demo.demo_ui import show_df\n",
    "from nexus_api import API\n",
    "from utils.data_model import Variable\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "from corr_analysis.graph.graph_utils import filter_on_signals\n",
    "from utils.io_utils import load_corrs_from_dir\n",
    "from demo.cluster_utils import CorrCommunity\n",
    "from demo.demo_ui import show_communities\n",
    "import pickle\n",
    "\n",
    "warnings. filterwarnings('ignore')\n",
    "\n",
    "use_qgrid = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Nexus API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = \"postgresql://yuegong@localhost/chicago_1m_zipcode\"\n",
    "nexus_api = API(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Data Assets\n",
    "\n",
    "You can use Nexus to browse data catalog and an individual dataset.\n",
    "\n",
    "### Download output data products\n",
    "\n",
    "Every dataframe displayed by Nexus is associated with a download button. After the button is clicked, the dataframe will be downloaded to the root directory of `nexus_correlation_discovery`. The name of the downloaded dataframe is the one you specified in the name field of `show_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show catalog\n",
    "catalog = nexus_api.show_catalog()\n",
    "show_df(catalog, name=\"catalog\", use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show original dataset\n",
    "dataset_id = 'divg-mhqk'\n",
    "df, link=nexus_api.show_raw_dataset(id=dataset_id)\n",
    "print(link)\n",
    "show_df(df, name=dataset_id, use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show aggregated dataset\n",
    "\n",
    "For example, 4u6w-irs9_location_6 is an aggreagted table, \n",
    "which is created over original table 4u6w-irs9 by aggregating its spatial attribute `location` \n",
    "to the zipcode granularity (zipcode granularity is mapped to 6 in Nexus).\n",
    "\"\"\"\n",
    "agg_tbl_name = '4u6w-irs9_location_6'\n",
    "df = nexus_api.show_agg_dataset(agg_tbl_name)\n",
    "show_df(df, name=agg_tbl_name, use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find correlations from an input table\n",
    "\n",
    "## Inputs:\n",
    "- `dataset`: dataset id\n",
    "- `temporal_granularity`: temporal granularity\n",
    "- `spatial_granularity`: spatial granularity\n",
    "- `overlap_threshold`: overlap threshold for joinable detection\n",
    "- `correlation_threshold`: correlation coefficient threshold\n",
    "- `correlation_type`: correlation type: `pearson`, `spearman`, `kendall`\n",
    "- `control_variables`: variables that you want to control for. When `control_variables` is specified, partial(conditional) correlations are calculated w.r.t control varaibles.\n",
    "\n",
    "## Outputs:\n",
    "A list of of correlations, and each correlation has the following attributes.\n",
    "\n",
    "- `table_id`: table id, `table_name`: table name\n",
    "\n",
    "- `agg_table`: the table name of the aggregated table. For example, 4u6w-irs9_location_6 is an aggreagted table, which is created over original table 4u6w-irs9 by aggregating its spatial attribute `location` to the zipcode granularity (zipcode granularity is mapped to 6 in Nexus).\n",
    "\n",
    "- `agg_attr`: the attribute after aggregation.\n",
    "\n",
    "- `correlation coefficient` is the correlation coefficient.\n",
    "\n",
    "- `p value` is the p value for the correlation\n",
    "\n",
    "- `original_attribute_missing_ratio` is the fraction of missing values in the original attribute before any aggregation.\n",
    "\n",
    "- `number of samples` is the number of rows used to calculate the correlation.\n",
    "\n",
    "- `spatio-temporal key type` indicates whether this correlation by spatial alignment or temporal alignment or both.\n",
    "\n",
    "\n",
    "In this example, the input is the asthma dataset. We align the input with tables from Chicago open data and compute correlations. Tables from chicago open data originally have the spatial granularity of geo-coordinate. We aggregate them to the zipcode level and apply aggregate functions \"avg\" and \"count\". For example, if you see an attribute named \"avg_basketball_courts\", it means the original attribute is \"basketball_courts\" and function \"avg\" is applied. The attribute after aggregation is named \"avg_basketball_courts\". In the displayed dataframe, you can perform sorting on one dimension, filtering rows using keywords, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'asthma'\n",
    "# asthma data only has spatial attribute, thus the temporal granularity is set to ALL.\n",
    "temporal_granularity, spatial_granularity = TEMPORAL_GRANU.ALL, SPATIAL_GRANU.ZIPCODE\n",
    "overlap_threshold = 5\n",
    "correlation_threshold = 0.5\n",
    "# you can change correlation_type to 'spearman' or 'kendall'\n",
    "df = nexus_api.find_correlations_from(dataset, temporal_granularity, spatial_granularity, \n",
    "                                      overlap_threshold, correlation_threshold, \n",
    "                                      correlation_type=\"pearson\")\n",
    "show_df(df, name='asthma_corrs', use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control for variables\n",
    "\n",
    "We got 227 correlations for the asthma dataset. After browsing several correlations, we realize that \"poverty\" might be driving these correlations after going through these correlations. Thus, we want to control for the income level of each zipcode when calculating correlations. To achieve that, users can specify variables that they want to control for in the `control_variables` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'asthma'\n",
    "temporal_granularity, spatial_granularity = TEMPORAL_GRANU.ALL, SPATIAL_GRANU.ZIPCODE\n",
    "overlap_threshold = 5\n",
    "correlation_threshold = 0.5\n",
    "control_variables = [Variable('chicago_income_by_zipcode_zipcode_6', 'avg_income_household_median')]\n",
    "df_control = nexus_api.find_correlations_from(dataset, temporal_granularity, spatial_granularity, \n",
    "                                              overlap_threshold, correlation_threshold, \n",
    "                                              correlation_type=\"pearson\", control_variables=control_variables)\n",
    "show_df(df_control, name='asthma_corrs_control_income', use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble a dataset from multiple variables\n",
    "\n",
    "1. df, prov = get_aligned_data(correlation): this api takes input as a correlation and returns the merged dataset used to calculate this correlation\n",
    "\n",
    "2. df, prov = assemble(vars, constraints: [optional]): this api creates a dataset that merges all variables specified in `vars`. `constaints` is a mapping between table name and the constraint on that table when performing join operation. For example {'tbl_A': 2} means spatio-temporal units with the number of samples smaller than 2 are discarded. \n",
    "\n",
    "data assembly APIs return `prov`, which is the provenance information of the resulting dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = 0\n",
    "aligned, prov = nexus_api.get_joined_data_from_row(df.loc[0])\n",
    "show_df(aligned, name=\"asthma_corrs_aligned\", prov=prov, use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without constraint\n",
    "vars = [Variable('divg-mhqk_location_6', 'count'), Variable('4u6w-irs9_location_6', 'avg_square_feet')]\n",
    "df, prov = nexus_api.join_and_project(vars)\n",
    "show_df(df, name=\"divg-mhqk_4u6w-irs9\", prov=prov, use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with constraint, units with number of samples smaller than 2 are dropped\n",
    "vars = [Variable('divg-mhqk_location_6', 'count'), Variable('4u6w-irs9_location_6', 'avg_square_feet')]\n",
    "constraints = {'divg-mhqk_location_6': 2, '4u6w-irs9_location_6': 2}\n",
    "df, prov = nexus_api.join_and_project(vars, constraints)\n",
    "show_df(df, name=\"divg-mhqk_4u6w-irs9_sample_greater_than_2\", prov=prov, use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "When you find multiple intriguing correlations and wish to conduct further regression analysis on variables of interest, you can begin by utilizing Nexus's `join_and_project` function to compile the necessary dataset. Subsequently, you may employ any data analysis library for regression analysis. In this instance, we will illustrate the process using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = Variable('asthma_Zip5_6', 'avg_enc_asthma')\n",
    "independent_variables = [Variable('ijzp-q8t2_location_6', 'count'), Variable('n26f-ihde_pickup_centroid_location_6', 'avg_tip')]\n",
    "\n",
    "data_to_analyze, provenance = nexus_api.join_and_project([dependent_variable] + independent_variables)\n",
    "\n",
    "# apply any data anlysis method\n",
    "regression_model = linear_model.LinearRegression() # OLS regression\n",
    "\n",
    "x = df[[variable.attr_name for variable in independent_variables]]\n",
    "y = df[dependent_variable.attr_name]\n",
    "model = regression_model.fit(x, y)\n",
    "r_squared = model.score(x, y)\n",
    "\n",
    "print(\"coefficients of each independent variables:\", model.coef_)\n",
    "print(\"r square score:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Correlations\n",
    "\n",
    "In this section, we will explain how to analyze output correlations in Nexus. We will use the correlations from chicago open data at the census tract and month granularity as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load correlations: corrs is a list of correlations; corr_map is map from correlated variables to their correlation coefficients\n",
    "corr_path = \"/home/cc/nexus_correlation_discovery/evaluation/correlations2/chicago_1m_T_GRANU.MONTH_S_GRANU.TRACT/\"\n",
    "corrs, corr_map = load_corrs_from_dir(corr_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Nexus Variable Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus searches for an optimal set of signals that, when applied as filters, yield a correlation graph with the highest modularity score. The signals that we consider for chicago open data include:\n",
    "\n",
    "- Missing value ratio in the aggregated column\n",
    "- Missing value ratio in the original column\n",
    "- Zero value ratio in the aggregated column\n",
    "- Zero value ratio in the original column\n",
    "- The absolute value of correlation coefficient\n",
    "- Overlap: number of samples used to calculate the correlation\n",
    "\n",
    "In chicago open data, the best set of thresholds for the above signals are [1.0, 1.0, 1.0, 0.8, 0.6, 70], which means we include correlations whose missing_ratio <= 1.0, missing_ratio_original<=1.0, zero_ratio <=1.0, zero_ratio_original <= 0.8, |r| >= 0.6, |samples| >= 70.`\n",
    "\n",
    "You can play with different set of thresholds as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_thresholds = [1.0, 1.0, 1.0, 0.8, 0.6, 70]\n",
    "corr_community = CorrCommunity(corrs, 'chicago')\n",
    "corr_community.get_correlation_communities_chicago(signal_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Correlation Communities\n",
    "\n",
    "We implement a simple interface for you to explore our correlation communities. Each community is composed of a group of variables. By default, the display is set to only show the tables where these variables are found. To view the specific variables within a community, simply click the \"Show Variables\" button.\n",
    "\n",
    "Clicking the \"Show Correlations\" button will reveal all the correlations within a community. Once displayed, you have the flexibility to apply any filters to the resulting dataframe.\n",
    "\n",
    "FAQ:\n",
    "\n",
    "Why do some communities display the exact same set of tables?\n",
    "\n",
    "The reason is that while the tables might be the same, the variables within these communities differ. We construct the correlation graph based on variables, and then present it in a table-view for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_communities(corr_community, show_corr_in_same_tbl=False, use_qgrid=use_qgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Factor Analysis\n",
    "\n",
    "Factor analysis aims to extract common factors from observed variables and represent existing variables using fewer factors. \n",
    "\n",
    "Factor analysis can take as input a correlation matrix. It derives factors that are essentially linear combinations of the observed variables. These factors are crafted to closely approximate the original correlation matrix when observed variables are projected onto them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to remove correlations that have values of 1 or -1 to avoid singular matrix\n",
    "corrs, corr_map = load_corrs_from_dir(corr_path, remove_perfect_corrs=True) \n",
    "signals = [1.0, 1.0, 1.0, 0.8, 0.6, 70] # we use the same signal thresholds as in the previous example\n",
    "corrs_filtered = filter_on_signals(corrs, None, signals)\n",
    "\n",
    "n_factors = 10 # set the number of factors to 10\n",
    "\n",
    "\"\"\"\n",
    "the following code fits a factor analysis model on the correlation matrix online\n",
    "It takes 10 minutes to run; save_path indicates the path to save the factor analysis model (fa)\n",
    "\"\"\"\n",
    "# fa, clusters = nexus_api.factor_analysis(corrs_filtered, corr_map, n_factors, save_path=\"chicago_open_data_factor_analysis.pkl\")\n",
    "\n",
    "\"\"\"\n",
    "For the purpose of this demo, we load the factor analysis model from the file \"chicago_open_data_factor_analysis.pkl\"\n",
    "\"\"\"\n",
    "fa = pickle.load(open(\"chicago_open_data_factor_analysis.pkl\", \"rb\"))\n",
    "clusters, covered_vars = nexus_api.build_factor_clusters(fa, corrs_filtered, corr_map, n_factors, threshold=0.5)\n",
    "corr_community = CorrCommunity(corrs_filtered, 'chicago', clusters)\n",
    "show_communities(corr_community, show_corr_in_same_tbl=False, use_qgrid=use_qgrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
