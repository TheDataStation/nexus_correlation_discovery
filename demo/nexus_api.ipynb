{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dtale\n",
    "home_dir=os.path.expanduser('~')\n",
    "os.chdir(f\"{home_dir}/nexus_correlation_discovery/\")\n",
    "from demo import nexus_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nexus Introduction\n",
    "\n",
    "Correlation analysis is a vital initial step for investigating causation, essential for understanding complex phenomena and making informed choices. While it is hard to establish causality from vast observational data without assumptions and expert knowledge, identifying correlations remains a key strategy to “cast a wide net” and detect potential causal links. Our system Nexus identifies correlations over collections of spatio-temporal tabular data, aiming to identify interesting hypotheses and provide a good starting point for further causal analysis. Nexus focuses on two personas.\n",
    "\n",
    "**Persona 1: Enrich an Existing Dataset.** A researcher at a medical school, Bob, has a dataset with asthma attack incidences in hospitals across various zip codes in Chicago. Bob's research goal is to explore what factors could potentially affect asthma attacks. Thus, he wants to start by finding variables that are correlated with asthma attacks. Persona 1 is someone who has an initial dataset and seeks to enrich such a dataset with additional variables relevant to the analysis.\n",
    "\n",
    "**Persona 2: Data-Driven Hypothesis Generation.** Amy, a social scientist in Chicago, is seeking to discover intriguing phenomena within the city for her research. To avoid limiting her analysis to existing knowledge, she employs a data-driven strategy. Recognizing that Chicago Open Data has a wealth of datasets on diverse societal aspects such as education, business, and crime, Amy wants to identify interesting correlations automatically to generate new hypotheses. Persona 2 has a large repository of tabular data and wants to automatically identify interesting correlations to formulate new hypotheses for further causal analysis.\n",
    "\n",
    "In this demonstration, we will illustrate how Nexus assists Persona 1 and 2 with the analysis of real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Nexus\n",
    "\n",
    "Let's first install Nexus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T22:58:00.810719834Z",
     "start_time": "2024-02-26T22:58:00.805785609Z"
    }
   },
   "outputs": [],
   "source": [
    "nexus_demo.install_nexus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Nexus API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexus.nexus_api import API\n",
    "conn_str = f'data/quickstart.db'\n",
    "nexus_api = API(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona 1: Enrich the asthma dataset with additional variables\n",
    "\n",
    "Bob, a researcher from a medical school, has a dataset with asthma attack incidences in hospitals across various zip codes in Chicago.\n",
    "\n",
    "| Zip5\\*   | enc_asthma\\*\\* | encAsthmaExac\\*\\*\\* | AttackPer\\*\\*\\*\\*  |\n",
    "|--------|------------|---------------|-----------|\n",
    "| 60604.0| 10.0       | 1.0           | 0.1       |\n",
    "| 60605.0| 47.0       | 7.0           | 0.15      |\n",
    "| 60606.0| 33.0       | 13.0          | 0.39      |\n",
    "| 60607.0| 12.0       | 3.0           | 0.25      |\n",
    "| ...| ...       | ...          | ...      |\n",
    "\n",
    "\\* zipcode\n",
    "\n",
    "\\*\\* Count of asthma visits 2009-2019, denominator.\n",
    "\n",
    "\\*\\*\\* Count of visits for asthma attacks (a.k.a., exacerbations) 2009-2019, numerator.\n",
    "\n",
    "\\*\\*\\*\\* Asthma attacks as a percentage of all asthma visits.\n",
    "\n",
    "Bob is searching for variables correlated with asthma attacks from external data sources. He finds that [Chicago Open Data](https://data.cityofchicago.org/) has a wealth of datasets on diverse societal aspects such as education, business, and crime in Chicago. He believes there are some variables in Chicago Open Data that are useful for his research. Thus, he adds Chicago Open Data as a data source in Nexus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Data Assets\n",
    "\n",
    "Now, Chicago Open Data has been added to Nexus and Bob can use Nexus to browse the data catalog. Note this data catalog contains both the original dataset and their aggregated version.\n",
    "\n",
    "For example, table `ijzp-q8t2` is Crimes - 2001 to Present. This table originally has geo-coordinate granularity. To combine it with the asthma dataset having zipcode granularity, Nexus automatically resolves the granularity inconsistency and creates table `ijzp-q8t2_location_6` that aggregates ijzp-q8t2 to the zipcode granularity using the `location` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = nexus_api.get_catalog()\n",
    "dtale.show(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Nexus to look at a dataset in the catalog given the dataset id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = '4u6w-irs9_location_6'\n",
    "df = nexus_api.get_agg_dataset(dataset_id)\n",
    "dtale.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find correlations from an input table\n",
    "\n",
    "Bob's goal is to explore what factors could potentially affect asthma attacks. Thus, he starts by finding variables that are correlated with asthma attacks. He can achieve this easily by using the `find_correlations_from` API in Nexus.\n",
    "\n",
    "In this API, Nexus aligns the asthma dataset with tables from Chicago Open Data and computes correlations. Tables from Chicago Open Data originally have the spatial granularity of geo-coordinate. We aggregate them to the zip code level and apply aggregate functions \"avg\" and \"count\". For example, if you see an attribute named `avg_basketball_courts`, it means the original attribute is `basketball_courts` and function `average` is applied. The attribute after aggregation is named `avg_basketball_courts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexus.utils.time_point import TEMPORAL_GRANU\n",
    "from nexus.utils.coordinate import SPATIAL_GRANU\n",
    "\n",
    "dataset = 'asthma'\n",
    "# asthma data only has spatial attribute, thus the temporal granularity is set to ALL.\n",
    "temporal_granularity, spatial_granularity = TEMPORAL_GRANU.ALL, SPATIAL_GRANU.ZIPCODE\n",
    "overlap_threshold = 5\n",
    "correlation_threshold = 0.5\n",
    "# you can change correlation_type to 'spearman' or 'kendall'\n",
    "correlations = nexus_api.find_correlations_from(dataset, temporal_granularity, spatial_granularity, \n",
    "                                      overlap_threshold, correlation_threshold, \n",
    "                                      correlation_type=\"pearson\")\n",
    "dtale.show(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexus.utils.time_point import TEMPORAL_GRANU\n",
    "from nexus.utils.coordinate import SPATIAL_GRANU\n",
    "\n",
    "dataset = 'ijzp-q8t2'\n",
    "temporal_granularity, spatial_granularity = TEMPORAL_GRANU.DAY, SPATIAL_GRANU.TRACT\n",
    "overlap_threshold = 5\n",
    "correlation_threshold = 0.5\n",
    "# you can change correlation_type to 'spearman' or 'kendall'\n",
    "correlations = nexus_api.find_correlations_from(dataset, temporal_granularity, spatial_granularity, \n",
    "                                      overlap_threshold, correlation_threshold, \n",
    "                                      correlation_type=\"pearson\")\n",
    "dtale.show(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the detailed profile of a correlation\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control for variables\n",
    "\n",
    "Bob got 234 correlations for the asthma dataset. After browsing several correlations, he realizes that \"poverty\" might be driving these correlations. Thus, we want to control for the income level of each zipcode when calculating correlations. To achieve that, users can specify variables that they want to control in the `control_variables` field. After controlling for the median household income in a zipcode, only 63 correlations are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexus.utils.data_model import Variable\n",
    "\n",
    "dataset = 'asthma'\n",
    "temporal_granularity, spatial_granularity = TEMPORAL_GRANU.ALL, SPATIAL_GRANU.ZIPCODE\n",
    "overlap_threshold = 5\n",
    "correlation_threshold = 0.5\n",
    "control_variables = [Variable('chicago_income_by_zipcode_zipcode_6', 'avg_income_household_median')]\n",
    "df_control = nexus_api.find_correlations_from(dataset, temporal_granularity, spatial_granularity, \n",
    "                                              overlap_threshold, correlation_threshold, \n",
    "                                              correlation_type=\"pearson\", control_variables=control_variables)\n",
    "dtale.show(df_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble a dataset from multiple variables\n",
    "\n",
    "Bob identifies a few interesting correlations and wants to combine variables involved in these correlations to assemble a new dataset. Nexus provides data assembly APIs to make it easy for Bob.\n",
    "\n",
    "Suppose Bob finds the first correlation intriguing and wishes to explore the data used to calculate it. In such a scenario, he can simply input the correlation's ID into Nexus to obtain the integrated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = 0\n",
    "aligned, prov = nexus_api.get_joined_data_from_row(df_control.loc[0])\n",
    "dtale.show(aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus also offers `join_and_project` API that can assemble a dataset from any set of given variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [Variable('divg-mhqk_location_6', 'count'), Variable('4u6w-irs9_location_6', 'avg_square_feet')]\n",
    "df, prov = nexus_api.join_and_project(variables)\n",
    "dtale.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus provides the data provenance information for all data assembly APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis\n",
    "\n",
    "When you find multiple intriguing correlations and wish to conduct further regression analysis on variables of interest, you can begin by utilizing Nexus's `join_and_project` function to compile the necessary dataset. Subsequently, you may employ any data analysis library for regression analysis. In this instance, we will illustrate the process using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "dependent_variable = Variable('asthma_Zip5_6', 'avg_enc_asthma')\n",
    "independent_variables = [Variable('ijzp-q8t2_location_6', 'count'), Variable('n26f-ihde_pickup_centroid_location_6', 'avg_tip')]\n",
    "\n",
    "data_to_analyze, provenance = nexus_api.join_and_project([dependent_variable] + independent_variables)\n",
    "# apply any data anlysis method\n",
    "regression_model = linear_model.LinearRegression() # OLS regression\n",
    "\n",
    "x = data_to_analyze[[variable.attr_name for variable in independent_variables]]\n",
    "y = data_to_analyze[dependent_variable.attr_name]\n",
    "model = regression_model.fit(x, y)\n",
    "r_squared = model.score(x, y)\n",
    "\n",
    "print(\"coefficients of each independent variables:\", model.coef_)\n",
    "print(\"r square score:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona 2: Data-Driven Hypothesis Generation.\n",
    "Amy, a social scientist in Chicago, is seeking to discover intriguing phenomena within the city for her research. To avoid limiting her analysis to existing knowledge, she employs a data-driven strategy. Recognizing that Chicago Open Data has a wealth of datasets on diverse societal aspects such as education, business, and crime, Amy wants to identify interesting correlations automatically to generate new hypotheses. \n",
    "\n",
    "She points Nexus to Chicago Open Data and uses the `find_all_correlations` API to identify all correlations within Chicago Open Data at the census tract and month granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexus.utils.time_point import TEMPORAL_GRANU\n",
    "from nexus.utils.coordinate import SPATIAL_GRANU\n",
    "chicago_correlations = nexus_demo.find_all_correlations(TEMPORAL_GRANU.MONTH, SPATIAL_GRANU.TRACT)\n",
    "print(f\"Nexus found {len(chicago_correlations)} correlations in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Nexus Variable Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus found 40,538 correlations in total, which is an overwhelming number for users to discern interesting correlations manually.\n",
    "\n",
    "Luckily, Nexus can distill the structure of correlations and extract a small number of variable clusters from the vast array of correlations. These variable clusters can help users identify causal links and confounders.\n",
    "\n",
    "Nexus searches for an optimal set of signals that, when applied as filters, yield a correlation graph with the highest modularity score. The signals that we consider for chicago open data include:\n",
    "\n",
    "- Missing value ratio in the aggregated column\n",
    "- Missing value ratio in the original column\n",
    "- Zero value ratio in the aggregated column\n",
    "- Zero value ratio in the original column\n",
    "- The absolute value of correlation coefficient\n",
    "- Overlap: number of samples used to calculate the correlation\n",
    "\n",
    "In chicago open data, the best set of thresholds for the above signals are [1.0, 1.0, 1.0, 0.8, 0.6, 70], which means we include correlations whose missing_ratio <= 1.0, missing_ratio_original<=1.0, zero_ratio <=1.0, zero_ratio_original <= 0.8, |r| >= 0.6, |samples| >= 70.`\n",
    "\n",
    "You can play with different sets of thresholds as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo.cluster_utils import CorrCommunity\n",
    "from demo.demo_ui import show_communities\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "def filter_on_signals(corr, signals, ts):\n",
    "    return corr[\n",
    "        (corr[\"missing_ratio1\"].values <= ts[0])\n",
    "        & (corr[\"zero_ratio1\"].values <= ts[1])\n",
    "        & (corr[\"missing_ratio2\"].values <= ts[0])\n",
    "        & (corr[\"zero_ratio2\"].values <= ts[1])\n",
    "        & (corr[\"missing_ratio_o1\"].values <= ts[2])\n",
    "        & (corr[\"zero_ratio_o1\"].values <= ts[3])\n",
    "        & (corr[\"missing_ratio_o2\"].values <= ts[2])\n",
    "        & (corr[\"zero_ratio_o2\"].values <= ts[3])\n",
    "        & (abs(corr[\"r_val\"]).values >= ts[4])\n",
    "        & (corr[\"samples\"].values >= ts[5])\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "def build_graph_on_vars(corrs, threshold=0, weighted=False):\n",
    "    G = nx.Graph()\n",
    "    labels = {}\n",
    "    from collections import defaultdict\n",
    "    tbl_attrs = defaultdict(set)\n",
    "    for _, row in corrs.iterrows():\n",
    "        tbl_id1, tbl_id2, tbl_name1, tbl_name2, agg_attr1, agg_attr2 = (\n",
    "            row[\"table_id1\"],\n",
    "            row[\"table_id2\"],\n",
    "            row[\"table_name1\"],\n",
    "            row[\"table_name2\"],\n",
    "            row[\"agg_attr1\"],\n",
    "            row[\"agg_attr2\"],\n",
    "        )\n",
    "        G.add_edge(f\"{tbl_id1}--{agg_attr1}\", f\"{tbl_id2}--{agg_attr2}\")\n",
    "        tbl_attrs[tbl_id1].add(agg_attr1)\n",
    "        tbl_attrs[tbl_id2].add(agg_attr2)\n",
    "        labels[f\"{tbl_id1}--{agg_attr1}\"] = f\"{tbl_name1}--{agg_attr1}\"\n",
    "        labels[f\"{tbl_id2}--{agg_attr2}\"] = f\"{tbl_name2}--{agg_attr2}\"\n",
    "\n",
    "    nx.set_node_attributes(G, labels, \"label\")\n",
    "    return G\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_communities(G):\n",
    "    random.seed(9)\n",
    "    # sort components by the number of variables in the cluster\n",
    "    comps = nx.community.louvain_communities(G, resolution=1)\n",
    "    print(len(comps))\n",
    "    all_communities = {}\n",
    "    for i, comp in enumerate(comps):\n",
    "        community = defaultdict(list)\n",
    "        for tbl_var in comp:\n",
    "            tbl_var = G.nodes[tbl_var][\"label\"]\n",
    "            x = tbl_var.split(\"--\")\n",
    "            tbl, var = x[0], x[1]\n",
    "            community[tbl].append(var)\n",
    "        all_communities[f\"Cluster {i}\"] = community\n",
    "    return all_communities, comps\n",
    "\n",
    "signal_thresholds = [1.0, 1.0, 1.0, 0.8, 0.6, 70]\n",
    "filtered_corr = filter_on_signals(chicago_correlations, None, signal_thresholds)\n",
    "G = build_graph_on_vars(filtered_corr, 0, False)\n",
    "communities, _ = get_communities(G)\n",
    "corr_community = CorrCommunity(chicago_correlations, 'chicago')\n",
    "corr_community.all_communities = communities\n",
    "# corr_community.get_correlation_communities_chicago(signal_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Correlation Communities\n",
    "\n",
    "We implement a simple interface for you to explore our correlation communities. Each community is composed of a group of variables. By default, the display is set to only show the tables where these variables are found. To view the specific variables within a community, simply click the \"Show Variables\" button.\n",
    "\n",
    "Clicking the \"Show Correlations\" button will reveal all the correlations within a community. Once displayed, you have the flexibility to apply any filters to the resulting dataframe.\n",
    "\n",
    "FAQ:\n",
    "\n",
    "Why do some communities display the exact same set of tables?\n",
    "\n",
    "The reason is that while the tables might be the same, the variables within these communities differ. We construct the correlation graph based on variables, and then present it in a table-view for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_communities(corr_community, show_corr_in_same_tbl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Factor Analysis\n",
    "\n",
    "Factor analysis aims to extract common factors from observed variables and represent existing variables using fewer factors. \n",
    "\n",
    "Factor analysis can take as input a correlation matrix. It derives factors that are essentially linear combinations of the observed variables. These factors are crafted to closely approximate the original correlation matrix when observed variables are projected onto them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to remove correlations that have values of 1 or -1 to avoid singular matrix\n",
    "corrs, corr_map = load_corrs_from_dir(corr_path, remove_perfect_corrs=True) \n",
    "signals = [1.0, 1.0, 1.0, 0.8, 0.6, 70] # we use the same signal thresholds as in the previous example\n",
    "corrs_filtered = filter_on_signals(corrs, None, signals)\n",
    "\n",
    "n_factors = 10 # set the number of factors to 10\n",
    "\n",
    "\"\"\"\n",
    "the following code fits a factor analysis model on the correlation matrix online\n",
    "It takes 10 minutes to run; save_path indicates the path to save the factor analysis model (fa)\n",
    "\"\"\"\n",
    "# fa, clusters = nexus_api.factor_analysis(corrs_filtered, corr_map, n_factors, save_path=\"chicago_open_data_factor_analysis.pkl\")\n",
    "\n",
    "\"\"\"\n",
    "For the purpose of this demo, we load the factor analysis model from the file \"chicago_open_data_factor_analysis.pkl\"\n",
    "\"\"\"\n",
    "fa = pickle.load(open(\"chicago_open_data_factor_analysis.pkl\", \"rb\"))\n",
    "clusters, covered_vars = nexus_api.build_factor_clusters(fa, corrs_filtered, corr_map, n_factors, threshold=0.5)\n",
    "corr_community = CorrCommunity(corrs_filtered, 'chicago', clusters)\n",
    "show_communities(corr_community, show_corr_in_same_tbl=False, use_qgrid=use_qgrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
